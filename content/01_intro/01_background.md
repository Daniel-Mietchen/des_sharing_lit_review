# Background

## Healthcare discrete-event simulation

In healthcare, discrete-event simulation (DES) is the most used computational method for modelling {cite}`salleh2017simulation, philip_2022, roy_2021, SALMON20181`. DES has proven useful within the field of health as it can be used to model patient care pathways, optimise health service delivery, investigate health queuing systems, and conduct health technology assessment. It has been applied to a wide variety of important clinical and health problems such as stroke care {cite:p}`Lahr_bmj`, emergency departments {cite:p}`Mohiuddine015007`, chronic obstructive pulmonary disease {cite}`Hoogendoorne049675`, sexual health {cite:p}`Mohiuddine_sexual_health_des`, reducing delayed discharges {cite:p}`ipacs_plos1`, mental health waiting times, critical care {cite:p}`penn_2020`, managing health services during the Covid-19 pandemic {cite}`Yakutcane062305`, and end of life care {cite:p}`Chalke043795`. Healthcare DES models are often complex research artefacts: they are time consuming to build, depend on specialist software, and logic may be difficult to describe accurately using words and diagrams alone {cite}`monks2019strengthening`. 

## Published computer models: study motivation

To enhance transparency of model logic, and offer others the potential to understand, learn from, or reuse a model, one option available to authors of healthcare DES studies is to openly publish the computer model.  We define a computer model to be either a model written in a specialist simulation software package, or model written in a general purpose programming language. The computer model is an artifact that is an implementation of the study conceptual model {cite}`robinson2014simulation`. It is an executable artifact and is used for experimentation.

The current extent of model sharing and practice of sharing DES computer models in the healthcare literature is unknown. To understand if and how authors of DES studies are sharing their models, draw lessons, and evaluate if this can be improved to benefit the wider community, we conduct a review of the contemporary DES literature between 2019 and 2022 inclusive. Reviews in other computational fields report that the sharing of model code and files has historically been low {cite}`Collberg_2016, stodden2018empirical, janssen2020code, brailsford_hybrid_2019, sterman_2012`. The closest of these field to our review in healthcare DES is in the field of Agent (or Individual) Based Simulation {cite}`janssen2020code`. This study examined 7500 articles reporting agent-based models and found only 11% of articles shared model code, although there was an upward trend: 18% of ABS publications were found to share their model in some form by 2018.

## Sharing models is a subset of reproducibility

Our focus in this study is on the practice of sharing healthcare DES computer models: to what extent do health researchers openly share their  DES computer models, how do they do it, and what actions could the DES community take to improve what is shared? We consider the open publication of models to be a subset of, and complementary to, the broader topic tackling the \textit{reproducibility} of computational analyses and modelling. There has been a long standing effort to provide incentives for authors to make their computational work reproducibile {cite}`marco_a_janssen_towards_2008, the_turing_way_community_2022_7470333, monks2019strengthening, TRACE_2021, heroux_editorial_2015, grimm_odd_2010, ruscheinski_artifact-based_2020, reinhardt_oddp_2018`. One of the most well known of these within the modelling and simulation community is the Association of Computing Machinery's (ACM) Reproducible Computational Results (RCR) initiative (https://www.acm.org/publications/policies/artifact-review-and-badging-current). The RCR is an optional extra peer review process for authors \textit{who publish in ACM journals}. Computational artifacts, i.e. models or algorithms, are peer reviewed by specialists and author publications are awarded badges based on the results. ACM RCR badges include: artifacts evaluated (as functional or reusable), artifacts available (deposited in a FORCE-11 compliant {cite}`smith_software_2016` archive such as the Open Science Framework) and Results Validated (either using the author provided artifacts or a higher level using independent methods). 

Initiatives such RCR are limited to specific journals, but health researchers publish, and may share DES models, in a wide variety of outlets. For example, mathematical, medical and clinical, Health Economic, health policy, and Operational Research journals; as well as specialist conferences that publish full peer reviewed papers (such as the Winter Simulation Conference). In these non RCR supported journals it is unlikely that model artifacts are peer reviewed. Those authors that share models may instead be guided by discipline norms, journal rules, open research guides such as the Turing Way {cite}`the_turing_way_community_2022_7470333`, or one of several DES reporting guidelines {cite}`monks2019strengthening, ZHANG2020506, ISPORSMDM`. 

The DES reporting guidelines take different approaches to publication of  DES computer models. The International Society for Pharmacoeconomics and Outcomes Research and the Society for Medical Decision Making (ISPOR-SDM; {cite}`ISPORSMDM`) encourage authors to make non-confidential versions of their computer models available to enhance transparency, but state that open models should not be a formal requirement of publication. The task-force note a number of reasons why code might not be able to be shared including intellectual property and cost. The Strengthening the Reporting of Empirical Simulation Studies (STRESS-DES; {cite}`monks2019strengthening`) guidelines takes the position that model code is an enhancement to transparency, not a requirement. The STRESS checklist asks for detailed information on the software environment and computing hardware used to execute the model.  Section 6 goes further and requires a *statement on how the computer model can be accessed*. This is intended to prompt authors to think about enhanced transparency, and enhance publication in journals that do not ask for "code and data availability statements". The reporting checklist developed by {cite}`ZHANG2020506` focuses only on logic and validation reporting, and does not prompt users for information on model code.  

One exception to the position of publication as an enhancement versus requirement for transparency is perhaps models tackling Covid-19. At the start of the coronavirus pandemic, the lack of transparency and access to epidemiological model code used to inform economic and public health policy contributed to public confusion and polarisation. This has lead to some calling for open publication of all model code related to any aspect of Covid-19 {cite}`covid19_transparency`.

## State-of the art practices for sharing computer model artifacts

The topic of sharing code, simulated experiments, computer model artifacts and the reproducibility of published results is a live topic in other computational fields such as neuroscience, life sciences and ecology {cite}`halchenko_four_2015, heil_reproducibility_2021, krafczyk_learning_2021, eglen_toward_2017,cadwallader_survey_2022`. Outside of the academic literature there are recent community driven guides, standards, and digital repositories. These include the Turing Way developed by the Alan Turing Institute {cite}`the_turing_way_community_2022_7470333`, the Open Modelling Foundation standards (https://www.openmodelingfoundation.org/), and the ability to deposit models using the Network for Computational Modeling in Social and Ecological Sciences (CoMSES Net; https://www.comses.net/). The state-of-the-art for sharing model artifacts is an emerging and evolving field; although in biostatistics it has been talked about as far back as 2011 {cite}`peng_reproducible_nodate`. Although this recent literature is disparate, when brought together the literature agrees on a number of practices which benefits the ability of others to find, access, reuse and freely adapt shared model artifacts.

Contemporary sharing of computer model artifacts is best done through a digital open science repository that has FORCE11 compliant citation {cite}`smith_software_2016` and guarantees on persistance of digital artifacts {cite}`lin_trust_2020`. Examples include Zenodo (https://zenodo.org/); Figshare (https://figshare.com/), the Open Science Framework (https://osf.io/) or CoMSES Net. Deposited models are provided with a permanent Digital Object Identifier (DOI) that can be used to cite the artifact. Researchers should already be familiar with DOIs as they are *minted* and allocated to published journal articles. An example is **10.1016/j.envsoft.2020.104873** that identifies an article by {cite}`janssen2020code`.  The advantage of this approach is that the exact code that is cited in the journal article is preserved (authors are free to work on new versions of the code). A related concept is that of the Open Researcher and Contributor Identifier (ORCID) {cite}`taylor2017open`. This is a unique identifier for an individual researcher. A trusted archive will accommodate ORCIDs within the meta-data of a deposited artifact: providing an unambiguous permanent link back to the authors of the artifact. This offers an improvement over e-mail addresses listed with a journal article that may become outdated shortly after publication. 

Published models should also be accompanied by an open license {cite}`heil_reproducibility_2021, halchenko_four_2015, eglen_toward_2017`. A license details how others may use or adapt the artifact(s), as well as re-share any adaptations and credit authors. At a minimum a license specifies the terms of use for a model, and waives the authors of any liability if the artifact is reused. There are many types of standard license to choose from.  For example, licensers of models might choose between a permissive type license (e.g. the MIT; or BSD 2-Clause) or a copyleft type license (e.g. GNU General Public License v3).  An alternative that is often used with open data, and open access publication, but also relevant for models are Creative Commons licenses such as the CC-BY 4.0 {cite}`taylor2017open`.  

Permissive and copyleft licenses are also used by DES packages developed using Free and Open Source Software (FOSS). Note that FOSS here is more than open source code. It grants the freedom for users to reuse, adapt and
distribute copies however they choose. Examples include R Simmer (GPL-2), SimPy (MIT) and JaamSim (Apache 2.0).  For an overview of FOSS packages for DES see {cite}`dagkakis2016review`.

To maximise the chances that another user can execute a computer artifact, a model's dependencies and the software environment must be specified {cite}`krafczyk_learning_2021, heil_reproducibility_2021`.  This can be challenging: many computational artifacts rely on other software that may be operating system specific. Formal methods exist to manage dependencies. Complexity can range from package managers, such as `conda` or `renv`, to containerisation (where a model, parameters, an operating system and dependencies are deployed via a container and software such as Docker), to Open Science Gateways that allow remote execution {cite}`taylor2017open`. Such methods may be best suited to computational artifacts written in code; for example a simulation package in Python, or R. Models developed in commercial Visual Interactive Modelling packages such as Arena or Simul8 rely on software with strict proprietary licensing stipulations (i.e. paid licenses), but the software and operating system versions can be reported within the meta-data of the deposited artifact. Several commercial simulation packages now provide cloud versions of their software where users may upload a computer model and allows others to execute it without installation. However, such tools do not adhere to the guarantees provided by a trusted digital repository such as Zenodo.

Execution of a computer model artifact should be guided by a clear set of instructions: for example the inclusion of a README file that includes an overview of what a model does, how to execute it, and how to vary parameters {cite}`eglen_toward_2017, cadwallader_survey_2022`. Documentation of models developed using code only could be enhanced with notebooks that combine code and explanation {cite}`TRACE_2021, the_turing_way_community_2022_7470333`. 

Lastly, if coded models are to be trusted, reused or adapted then some form of testing and verification should be included with the shared model {cite}`the_turing_way_community_2022_7470333`. Test driven development is one option for the simulation community {cite}`ONGGO2016517`.

## Time, effort, and alternatives
The state-of-the-art methods and benefits outlined above do come at the cost of time and effort. Publishing a computer model artifact along with a journal article may also prompt authors to clean up code and models ready for sharing. There is some evidence that the time authors are willing to spend on this varies with experience; with more established authors being willing to spend more time than those with fewer publications {cite}`cadwallader_survey_2022`.  Authors may choose to adopt one or a combination of the practices recommended by the literature. More complex methods require more effort.  For example, in a small trial the journal {cite}`Nature Biotechnology` report that it took a median of nine days for authors to setup an online executable version of their computational artifacts using the containerisation and compute services provided by Code Ocean {cite}`nature_bio_changing_2019`.  In contrast, depositing code or a model in a trusted digital archive such as Zenodo requires the only the time to upload the data and effort to add meta-data such as ORCIDs. 

A simple alternative option to direct publication of computer models is to use a Data Availability Statement (DAS).  A DAS provides a way for authors to describe how others might access the computational artifacts used within the research. For example, "the materials used within this study are available from the corresponding author on reasonable request". A substantial downside is that DAS statements offering to share are frequently not honoured, even in journals mandating reproducibility standards {cite}`anssen2020code, gabelica_many_2022, Collberg_2016, stodden2018empirical`.  In the largest simulation review to date the study researchers contacted all authors of ABS papers that included a DAS within their paper. They received a response from less than 1% of authors to provide their code; the majority of these indicated that their model is no longer available, or failed to provide a runnable version {cite}`janssen2020code`. Outside of simulation modelling other disciplines have reported varying results when contacting authors of papers with DAS statements, with positive responses of 7% {cite}`gabelica_many_2022`,  33% {cite}`Collberg_2016`, and 44% {cite}`stodden2018empirical`. 


## References

```{bibliography}
:filter: docname in docnames
```